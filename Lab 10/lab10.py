""" Testing Table:
+ - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - +
+ For this lab, I tested numbers 1 - 1000 using a for loop in intervals of 10. 
  Here are some of the values generated below and the max value of these at the end.
  These numbers tended to alternate on every test run. 
  These are from 1 run of this program.

[n_estimators, x/y test]
1, 0.672
11, 0.918
21, 0.901
51, 0.901
201, 0.836
501, 0.80
741, 0.819
941, 0.934 (max value)
991, 0.836

While the values were not consistent, the values tended to increase as the n_estimators
increased, meaning there is a random but slighlty correlated trend of increasing between
these two values.
    
    
    
    
"""

from random import randint
import pandas as pd
import numpy as np
heart_disease = pd.read_csv('/Users/cgrab/OneDrive/Desktop/heart.csv')

X = heart_disease.drop(['target'] , axis=1) 
Y = heart_disease['target']

maxVal = {}

from sklearn.ensemble import RandomForestClassifier 

for i in range(1, 1000, 10):
    clf = RandomForestClassifier(n_estimators=i)

    from sklearn.model_selection import train_test_split

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)
    
    clf.fit(X_train, Y_train)
    y_pred = clf.predict(X_test)

    print(str(i), end = " ")
    #print(clf.score(X_train, Y_train))
    greatest = clf.score(X_test, Y_test)
    print(greatest)
    maxVal[i] = greatest
    
max_value = max(maxVal.values()) # maximum value
print(max_value)
max_index = [key for key in maxVal if maxVal[key] == max_value]

print("\nValues:")
print(max_value)
print(max_index)

"""
Below is the full output of the program for a single run. This is summerized above.

1 0.6721311475409836
11 0.9180327868852459
21 0.9016393442622951
31 0.7704918032786885
41 0.8360655737704918
51 0.9016393442622951
61 0.8032786885245902
71 0.8360655737704918
81 0.8524590163934426
91 0.819672131147541
101 0.7868852459016393
111 0.8032786885245902
121 0.8360655737704918
131 0.9180327868852459
141 0.8360655737704918
151 0.819672131147541
161 0.9016393442622951
171 0.7540983606557377
181 0.8032786885245902
191 0.9016393442622951
201 0.8360655737704918
211 0.8032786885245902
221 0.8360655737704918
231 0.8360655737704918
241 0.8032786885245902
251 0.7868852459016393
261 0.819672131147541
271 0.8032786885245902
281 0.7704918032786885
291 0.8360655737704918
301 0.8360655737704918
311 0.7377049180327869
321 0.8360655737704918
331 0.8360655737704918
341 0.819672131147541
351 0.819672131147541
361 0.8524590163934426
371 0.819672131147541
381 0.7868852459016393
391 0.8360655737704918
401 0.8032786885245902
411 0.8360655737704918
421 0.8524590163934426
431 0.9016393442622951
441 0.8360655737704918
451 0.7704918032786885
461 0.8360655737704918
471 0.7704918032786885
481 0.819672131147541
491 0.9180327868852459
501 0.8032786885245902
511 0.8524590163934426
521 0.8524590163934426
531 0.8360655737704918
541 0.819672131147541
551 0.8032786885245902
561 0.8852459016393442
571 0.8524590163934426
581 0.7540983606557377
591 0.9016393442622951
601 0.8032786885245902
611 0.8688524590163934
621 0.8032786885245902
631 0.819672131147541
641 0.7540983606557377
651 0.7704918032786885
661 0.819672131147541
671 0.8360655737704918
681 0.8524590163934426
691 0.9016393442622951
701 0.8524590163934426
711 0.8852459016393442
721 0.8032786885245902
731 0.7704918032786885
741 0.819672131147541
751 0.8524590163934426
761 0.8360655737704918
771 0.8688524590163934
781 0.819672131147541
791 0.8032786885245902
801 0.819672131147541
811 0.7704918032786885
821 0.7868852459016393
831 0.8360655737704918
841 0.8688524590163934
851 0.8360655737704918
861 0.8360655737704918
871 0.8360655737704918
881 0.7704918032786885
891 0.8688524590163934
901 0.8524590163934426
911 0.819672131147541
921 0.8360655737704918
931 0.8688524590163934
941 0.9344262295081968
951 0.7213114754098361
961 0.819672131147541
971 0.8688524590163934
981 0.819672131147541
991 0.8360655737704918
0.9344262295081968

Values:
0.9344262295081968
[941]

"""

